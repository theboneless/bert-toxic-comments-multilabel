{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of BERT NLP TRIAL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOhrHdx0DxpXjgLatZAAqt8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theboneless/bert-toxic-comments-multilabel/blob/master/Copy_of_BERT_NLP_TRIAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsnbD_jKNCH0",
        "colab_type": "code",
        "outputId": "ea95abef-93c4-4f47-bf01-8a15418b9be4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmWunDeuOWMz",
        "colab_type": "code",
        "outputId": "ee860164-5622-4832-e6e2-636c92497a59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.23)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.23 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.23)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiPOXYuiP5kf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG4xsoybOwZU",
        "colab_type": "code",
        "outputId": "96056f20-cf41-4180-cfac-66fbf741f4cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        }
      },
      "source": [
        "dataset = pd.read_csv('/Tweets.csv')\n",
        "dataset.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cairdin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:35:52 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:59 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>570301083672813571</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yvonnalynn</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:48 -0800</td>\n",
              "      <td>Lets Play</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>570301031407624196</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Bad Flight</td>\n",
              "      <td>0.7033</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:36 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>570300817074462722</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Can't Tell</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:14:45 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             tweet_id  ...               user_timezone\n",
              "0  570306133677760513  ...  Eastern Time (US & Canada)\n",
              "1  570301130888122368  ...  Pacific Time (US & Canada)\n",
              "2  570301083672813571  ...  Central Time (US & Canada)\n",
              "3  570301031407624196  ...  Pacific Time (US & Canada)\n",
              "4  570300817074462722  ...  Pacific Time (US & Canada)\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47tIZguSP8Zw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = dataset.drop(['airline_sentiment_confidence', 'tweet_id','negativereason', 'negativereason_confidence', 'airline_sentiment_gold', 'name', 'negativereason_gold', 'retweet_count', 'tweet_coord', 'tweet_created','user_timezone'], axis = 1 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-8ZI5u1QDKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = dataset.drop(['airline', 'tweet_location'], axis = 1 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLaFor3bRJTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# converting the label into numerical categories\n",
        "#y_int = []\n",
        "#from sklearn import preprocessing\n",
        "#le = preprocessing.LabelEncoder()\n",
        "#le.fit(y_string)\n",
        "#y_int = le.transform(y_string)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mbhyF5oQRCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentences = dataset.text.values\n",
        "labels = dataset.airline_sentiment.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK3GRSjjRWRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# converting the label into numerical categories\n",
        "y_int = []\n",
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(labels)\n",
        "y_int = le.transform(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8iSS0H_SB-X",
        "colab_type": "code",
        "outputId": "a8b576bf-3fa1-4ea2-d796-60bc7fb94444",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz7wCaBTSZ68",
        "colab_type": "code",
        "outputId": "fc56b860-2f69-495e-f8f8-8056a1fc1646",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  @VirginAmerica What @dhepburn said.\n",
            "Tokenized:  ['@', 'virgin', '##ame', '##rica', 'what', '@', 'dh', '##ep', '##burn', 'said', '.']\n",
            "Token IDs:  [1030, 6261, 14074, 14735, 2054, 1030, 28144, 13699, 8022, 2056, 1012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQCnToysSlQ6",
        "colab_type": "code",
        "outputId": "cf237a05-2a93-400f-f5ca-a8de84caae74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  @VirginAmerica What @dhepburn said.\n",
            "Token IDs: [101, 1030, 6261, 14074, 14735, 2054, 1030, 28144, 13699, 8022, 2056, 1012, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewbLB_RcSpFy",
        "colab_type": "code",
        "outputId": "df3d8dac-0bb0-44cc-96c5-089502fd6dfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  67\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPkXIzD5SuWV",
        "colab_type": "code",
        "outputId": "c08e510c-a1eb-4e3b-8294-003f241afec5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 68\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 68 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GO2EzuN1S5ND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWlLgfTUS_3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, y_int, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEwk6JQGTelL",
        "colab_type": "code",
        "outputId": "efff9eba-9dcc-4397-b75b-1e522efb07b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(train_labels)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 ... 0 2 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90pXbLVqTLAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op0wnhvKUYuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j07XxbwKUe2U",
        "colab_type": "code",
        "outputId": "f79a3458-29d2-46f9-c7c1-dc8bc34552e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zCnvNKVU1hD",
        "colab_type": "code",
        "outputId": "f63f35f5-eb8c-4831-8d0e-c685cba7389c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        }
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (3, 768)\n",
            "classifier.bias                                                 (3,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7eEeXrtUdLT",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCyjeBUWVH35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kkf3Qfh4VqdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6TMi4WdVJc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTH1O0Z4VN6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjyqRw6iVVNl",
        "colab_type": "code",
        "outputId": "4ea3d8a6-d8b6-4786-ad4d-4b61a6cd5f47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    412.    Elapsed: 0:00:18.\n",
            "  Batch    80  of    412.    Elapsed: 0:00:36.\n",
            "  Batch   120  of    412.    Elapsed: 0:00:53.\n",
            "  Batch   160  of    412.    Elapsed: 0:01:11.\n",
            "  Batch   200  of    412.    Elapsed: 0:01:29.\n",
            "  Batch   240  of    412.    Elapsed: 0:01:47.\n",
            "  Batch   280  of    412.    Elapsed: 0:02:06.\n",
            "  Batch   320  of    412.    Elapsed: 0:02:24.\n",
            "  Batch   360  of    412.    Elapsed: 0:02:42.\n",
            "  Batch   400  of    412.    Elapsed: 0:03:00.\n",
            "\n",
            "  Average training loss: 0.50\n",
            "  Training epcoh took: 0:03:06\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    412.    Elapsed: 0:00:18.\n",
            "  Batch    80  of    412.    Elapsed: 0:00:37.\n",
            "  Batch   120  of    412.    Elapsed: 0:00:55.\n",
            "  Batch   160  of    412.    Elapsed: 0:01:13.\n",
            "  Batch   200  of    412.    Elapsed: 0:01:31.\n",
            "  Batch   240  of    412.    Elapsed: 0:01:50.\n",
            "  Batch   280  of    412.    Elapsed: 0:02:08.\n",
            "  Batch   320  of    412.    Elapsed: 0:02:26.\n",
            "  Batch   360  of    412.    Elapsed: 0:02:45.\n",
            "  Batch   400  of    412.    Elapsed: 0:03:03.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epcoh took: 0:03:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    412.    Elapsed: 0:00:18.\n",
            "  Batch    80  of    412.    Elapsed: 0:00:37.\n",
            "  Batch   120  of    412.    Elapsed: 0:00:55.\n",
            "  Batch   160  of    412.    Elapsed: 0:01:13.\n",
            "  Batch   200  of    412.    Elapsed: 0:01:32.\n",
            "  Batch   240  of    412.    Elapsed: 0:01:50.\n",
            "  Batch   280  of    412.    Elapsed: 0:02:08.\n",
            "  Batch   320  of    412.    Elapsed: 0:02:26.\n",
            "  Batch   360  of    412.    Elapsed: 0:02:45.\n",
            "  Batch   400  of    412.    Elapsed: 0:03:03.\n",
            "\n",
            "  Average training loss: 0.21\n",
            "  Training epcoh took: 0:03:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    412.    Elapsed: 0:00:18.\n",
            "  Batch    80  of    412.    Elapsed: 0:00:37.\n",
            "  Batch   120  of    412.    Elapsed: 0:00:55.\n",
            "  Batch   160  of    412.    Elapsed: 0:01:13.\n",
            "  Batch   200  of    412.    Elapsed: 0:01:31.\n",
            "  Batch   240  of    412.    Elapsed: 0:01:50.\n",
            "  Batch   280  of    412.    Elapsed: 0:02:08.\n",
            "  Batch   320  of    412.    Elapsed: 0:02:26.\n",
            "  Batch   360  of    412.    Elapsed: 0:02:45.\n",
            "  Batch   400  of    412.    Elapsed: 0:03:03.\n",
            "\n",
            "  Average training loss: 0.14\n",
            "  Training epcoh took: 0:03:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qES2aPwQY8sT",
        "colab_type": "code",
        "outputId": "4667b8b5-20bd-46de-ab30-4aec1f5be711",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvAAAAGaCAYAAABpIXfbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9GTuBcdZC2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}